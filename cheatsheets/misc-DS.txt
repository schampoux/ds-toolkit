Outliers: 
    This term does not have a precise standard definition. Many people define it using the innerquartile range (IQR)
    IQR - The difference between the 75th and 25th percentiles. 
        - The value at the 75% point (.quantile(.75)) minus the value at the 25% point
    Outliers are then values below the 25% point - 1.5*IQR or values above the 75% + 1.5*IQR

np.nan
    - float 
    - not equal to itself
    - cant be converted into an int
    - search for them using `isnan()` in NumPy

*------------------------
Neural Networks 
*------------------------

    Neuron: 
        At its core, it is a linear transformation of the input followed by the application of a fixed nonlinear function (activation function)
        o = f(w*x+b)
            o as output
            x as input (scalar or vectorvalues, i.e. holding many scalar values scalars)
            f as an activation function, set to the hyperbolic tangent, or tanh function. 
            w as a learned parameter (single scalar or matrix) 
            b as a learned parameter (scalar or vector) Matching dimensionality to x
        In a multidimensional scenario (for w & b), his expression is referred to as a layer of neurons. 

    Activation Function: 
        In the inner parts of the model, it allows the output function to have different slopes at different values - something a linear function by definition cannot do. 
        By cleverly composing these differently sloped parts for many outputs, neural networks can approximate arbitrary functions. 
        At the last layer of the network, it has the role of concentrating the outputs of the preceding linear operation into a given range. 

        Activation functions are nonlinear, and differentiable (so that gradients can be computed through them. )
        These functions have at least one sensitive range, where nontrivial changes to the input result in a corresponding nontrivial change to the output.
            This is needed for training 
        Many of them have an insensitive (or saturated) range, where changes to the input result in little or no change to the output. 
        
        Often, but not always Activation functions will have at least one of the following: 
            a lower bound that is approached (or met) as the input goes to negative infinity
            a similar-but-inverse upper bound for positive infinity

        A neural network needs atleast one hidden layer of activations. 
            The simplest neural network is 
                linear
                Activation
                linear
            The final layer will take the output of activations and combine them linearly, to produce the output value. 
            
    Normalization 
        It is good practice to normalize the dataset so that each channel has zero mean and unitary standard deviation. 
            By choosing activation functions that are linear around `0` plus or minus `1`(or`2`), 
                keeping the data in the same range means it's more likely that neurons have nonzero gradients and hence will learn sooner. 

The combination of using `nn.LogSoftmax()` and `nn.NLLLoss()` is equivalent to using `nn.CrossEntropyLoss`. This terminology is a particularity of Pytorch 
    This is because the `nn.NLLLoss()` computes the cross entropy but with log probability predictions as inputs where `nn.CrossEntropyLoss` takes scores (logits). 
    Technically, nn.NLLLoss is the cross entropy between the Dirac distribution, putting all mass on the target, and the predicted distribution given by the log probability inputs. 



Convolution / Discrete cross-correlations    *Look this up and explain it as an exercise*
    Output Channels = Convolution Filters 

    Convolutions allow us to have smaller model looking for local patterns, whose weights are optimized across the entire image. 
    Equivalent to having multiple linear operations whose weights are zero almost everywhere except around individual pixels and that receive equal updates during training. 
        nn.Conv1d for time series 
        nn.Conv2d for images
        nn.conv3d for volumes or videos 
    
    Note, Applying a convolution kernel as a weighted sum of pixels in a 3 x 3 neighborhood requires that there are neighbors in all directions. 
    If we are at index 00, we only have pixels to the right and below. By default, pytorch will slide the convolution kernel within the input picture, 
        getting `width - kernel_width + 1`horizontal and vertical positions. 
    Pytorch gives us the possibility of padding the image by createing ghost pixels around the border that have value zero as far as the convolution is concerned. 
    Why use padding? 
        Helps us separate the matters of convolution and changing image sizes so we have one less thing to remember. 
        when we have more elaborate structures (skip connections, U-Nets) we want the tensors before and after a few convolutions to be of compatible size so that we can add them or take differences. 

    Although weights and biases are parameters that are learned through backpropogation, with convolutions 2we can set them by hand to see what happens. 

    MaxPool2d. 
        -explain as exercise 

nn.Module 
    A Module is a container for state in the forms of Parameters and submodules combined with the instrucitons to do a forward. 
    Stateless functions like activation and pooling can be used in the forward with torch.nn.functional rather than defined in the constructur. 
        With quantization, these stateless parts are now stateful because information about the quantization needs to be captured. 
    


The width of the network is the number of neurons per layer, or channels per convolution. The numbers specifying channels and features for each layer are directly related to the number of parameters in a model; 
    all other things being equal, they increase the capacity of the model. 
    The greater the capacity of the model, the more variability in the inputs the model will be able to manage; 
        likewise, the more likely overfitting will be since the model can use a great number of parameters to memorize unessential aspects of the input. 
    Combatting overfitting can be done by increasing sample size or augmenting existing data through artificial modifications of the same data. 

Regularization
    The mathematical tools aimed at easing model optimization as well as generalization. 
        - Optimization being the decrease in loss in the training set
        - Generalization being the validation set accuracy. 

    Weight Penalties:
        - Stabilizing generalization by adding a regularization term to the loss. 
        - A penalty on larger weight values, making the loss have a smoother topology
            Simultaneously less gain from fitting individual samples. 

        The following two regularization terms are scaled by a small factor, a hyperparameter we set prior to training. 
            L2 (weight decay): Sum of squares of all weights in the model
                Adding L2 regularization to the loss function is equivalent to decreasing each weight by an amount proportional to its current value during optimization step. 
            L1: Sum of the absolute values of all weights in the model. 
        
            Note that weight decaying applies to all parameters of the network, such as biases. 

        Note, the SGD optimizer in PyTorch already has a weight_decay parameter that corresponds to 2 * lambda, 
            and it directly performs weight decay during the opdate. 
            It is full equivalent to adding the L2 norm of weights to the loss, 
            without the need for accumulating terms in the loss and involving autograd. 
