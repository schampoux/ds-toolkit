{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19981dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim \n",
    "from torchvision import datasets, transforms \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f42154",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./datasets/\"\n",
    "cifar10 = datasets.CIFAR10(root = data_path, \n",
    "                           train=True, \n",
    "                           download=False,\n",
    "                           transform = transforms.Compose(\n",
    "                               [transforms.ToTensor(), \n",
    "                                transforms.Normalize(mean = (0.4914, 0.4822, 0.4465), std=(0.2470, 0.2435, 0.2616))]\n",
    "                           ))\n",
    "cifar10_val = datasets.CIFAR10(root = data_path,\n",
    "                               train=False,\n",
    "                               download=False,\n",
    "                               transform = transforms.Compose(\n",
    "                                   [transforms.ToTensor(), \n",
    "                                    transforms.Normalize(mean = (0.4914, 0.4822, 0.4465), std=(0.2470, 0.2435, 0.2616))]\n",
    "                            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1019a9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane', 'bird']\n",
    "label_map = {0:0, 2:1}\n",
    "\n",
    "cifar2 = [(img, label_map[label])for img, label in cifar10 if label in [0,2]]\n",
    "cifar2_val = [(img, label_map[label]) for img, label in cifar10_val if label in [0,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121afa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders \n",
    "train_loader = torch.utils.data.DataLoader(dataset = cifar2, batch_size=64, shuffle=True, pin_memory=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset = cifar2_val, batch_size=64, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8071dde",
   "metadata": {},
   "source": [
    "At a minimum, the arguments we provide to the Conv2d layer are the number of input features (channels of our multichannel images) The number of output features, and the size of the kernel. The more channels in the output image, the more the capacity of the network. Let's start with a kernel size of 3x3. Its common to have kernel sizes that are the same in all directions, so specifying a single value for the kernel argument will be interpreted as such. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f2577a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(3, 16, kernel_size=3)\n",
    "conv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f827d021",
   "metadata": {},
   "source": [
    "What do we expect for the shape of the weight tensor? \n",
    "\n",
    "- Kernel size is `3 x 3` so we want the weight to consist of `3 x 3` parts. \n",
    "- For a single output pixel value, our kernel would conside `3` input channels. \n",
    "- Therefore the weight component for a single output pixel value would be `in_ch * 3 * 3`\n",
    "- We have as many of these as we have output channels, which we have specified as `16`\n",
    "- The shape of our weight tensor will be `out_ch * in_ch * 3 * 3`\n",
    "- Finally, we have a bias term for each output channel. Remember this is a constant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53d9fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv.weight.shape, conv.bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddf78ad",
   "metadata": {},
   "source": [
    "Great, now we have smaller models looking for local patterns, whose weights are optimized across the entire image. \n",
    "\n",
    "A 2d convolution pass produces a 2D image as output, whose pixels are a weighted sum over neighborhoods of the input image. Let's pass an image through this layer just as we did when first introduced to linear layers. Note, `nn.Conv2d` expects the following input dimensions (B x C x H x W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b84e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = cifar2[0]\n",
    "output = conv(img.unsqueeze(0))\n",
    "img.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03069f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(output[0,0].detach(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473f744c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img.unsqueeze(0)[0,0].detach(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78615c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add padding \n",
    "img, label = cifar2[0]\n",
    "img = img.unsqueeze(0)\n",
    "\n",
    "conv = nn.Conv2d(3, 1, kernel_size=3, padding=1)\n",
    "output = conv(img)\n",
    "img.shape, output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7e665e",
   "metadata": {},
   "source": [
    "Experimenting with deliberately set weights and biases within our convolution layer. \n",
    "\n",
    "- bias=0\n",
    "- weights= constant value, so each pixel in the output gets the mean of its neighbors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d09e69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    conv.bias.zero_()\n",
    "with torch.no_grad():\n",
    "    conv.weight.fill_(1.0 / 9.0)\n",
    "output = conv(img)\n",
    "plt.imshow(output[0,0].detach(), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cbf6c9",
   "metadata": {},
   "source": [
    "Remember, every pixel of the output is the average of a neighborhood of the input, so pixels in the output are correlated and change more smoothly, thus a blurred image. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b1076d",
   "metadata": {},
   "source": [
    "Next we construct a vertical edge detection kernel. \n",
    "\n",
    "- 'detection' meaning the output has a high magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d49931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# edge detection kernel \n",
    "conv_e = nn.Conv2d(3, 1, kernel_size=3, padding=1)\n",
    "with torch.no_grad():\n",
    "    conv_e.weight[:] = torch.tensor([[-1.0, 0.0, 1.0],\n",
    "                                   [-1.0, 0.0, 1.0],\n",
    "                                   [-1.0, 0.0, 1.0]])\n",
    "    conv_e.bias.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324bf02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = conv_e(img) \n",
    "print(\"OUTPUT VERTICAL EDGE DETECTION KERNEL\")\n",
    "plt.imshow(output[0,0].detach(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3617b0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = conv_e(img) \n",
    "print(\"INPUT\")\n",
    "plt.imshow(img[0,0].detach(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b68aaf",
   "metadata": {},
   "source": [
    "### Downsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4a06a5",
   "metadata": {},
   "source": [
    "- Convolutions have helped solve the translation invariace issue, but we need to introduce downsampling and pooling, which will help us recognize not only subsections of the image, but the full image itself. \n",
    "- Combining convolutions and downsampling can help us recognize larger structured. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9518eece",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = nn.MaxPool2d(2)\n",
    "output = pool(img) \n",
    "output.shape, img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be8d172",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "\n",
    "    # First conv layer produces 16 independent (output) features that operate to (hopefully) discriminate low-level features \n",
    "    nn.Conv2d(3, 16, kernel_size=3, padding=1), \n",
    "\n",
    "    # activation \n",
    "    nn.Tanh(),\n",
    "\n",
    "    # Reduce 16-channel 32 x 32 image to a 16-channel 16 x 16 image\n",
    "    # downsample by half the size -> 4 x 4\n",
    "    nn.MaxPool2d(2),\n",
    "\n",
    "    # Produces an 8-channel 16 x 16 output. (intended to extract high level features) \n",
    "    nn.Conv2d(16, 8, kernel_size=3, padding=1),\n",
    "\n",
    "    # activation \n",
    "    nn.Tanh(),\n",
    "\n",
    "    # pool to an 8-channel, 8 x 8 output\n",
    "    # downsample by half the size -> 2 x 2\n",
    "    nn.MaxPool2d(2),\n",
    "\n",
    "    # ... Reshape from 8-channel 8 x 8 to a 1d 512 vector (batch x 512 technically)\n",
    "\n",
    "    nn.Linear(8*8*8, 32), # 532\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(32, 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d32d066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters \n",
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "print(\"Total Parameters: \", sum(numel_list),\"\\nParameters Per Layer: \",numel_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "610e51e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18090, [432, 16, 1152, 8, 16384, 32, 64, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn \n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        # input 3C 32x32  \n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1) # -> 16C 32x32 \n",
    "        self.act1 = nn.Tanh()\n",
    "        self.pool1 = nn.MaxPool2d(2), # -> 16C 16x16\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1) # 8C 16x16\n",
    "        self.act2 = nn.Tanh()\n",
    "        self.pool2 = nn.MaxPool2d(2) # -> 8C 8x8\n",
    "        # view(-1, 512) see forward()\n",
    "        # -> batch x 512\n",
    "        self.fc1 = nn.Linear(8*8*8, 32) # -> batch x 32\n",
    "        self.act3 = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(32, 2) # -> batch x 2\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.pool1(self.act1(self.conv1(x)))\n",
    "        out = self.pool2(self.act2(self.conv2(out)))\n",
    "        out = out.view(-1, 8*8*8)\n",
    "        out = self.act3(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "model = Net() \n",
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20cccf8",
   "metadata": {},
   "source": [
    "Note that when we register submodules in the constructor of our class, we are making sure we have access to their parameters. But what about submodules that have no parameters? (activation and pooling layers). There is no need to register these as submodules, rather we can call them in the forward using `torch.nn.functional`. By 'functional' we mean having no internal state. In other words, the output is strictly dependent on the input, unlike layers with parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1baa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        # input 3C 32x32  \n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1) # -> 16C 32x32 \n",
    "        # activation function here\n",
    "        # pool here -> 16C 16x16\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1) # 8C 16x16\n",
    "        # activation function here \n",
    "        # pool here -> 8C 8x8\n",
    "        # view(-1, 512) -> batch x 512\n",
    "        self.fc1 = nn.Linear(8*8*8, 32) # -> batch x 32\n",
    "        # activation function here\n",
    "        self.fc2 = nn.Linear(32, 2) # -> batch x 2\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(x)), 2)\n",
    "        out = out.view(-1, 8*8*8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CMI-BFRB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
