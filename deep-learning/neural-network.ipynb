{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4eff98a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import torch\n",
    "# commas every 000, .2 float precision\n",
    "pd.set_option('display.float_format', lambda x: f\"{x:,.2f}\")\n",
    "\n",
    "# Sets 1k and 1m instead of 1000 and 1000000.00\n",
    "# pd.set_eng_float_format(use_eng_prefix=True)\n",
    "\n",
    "# Sets accuracy after the decimal point \n",
    "# pd.set_eng_float_format(accuracy = 2)\n",
    "\n",
    "# Remove custom formatting and go back to default \n",
    "\n",
    "# pd.reset_option(\"display.float_format\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "030c6a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "t_c = [0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0]\n",
    "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n",
    "t_c = torch.tensor(t_c) #celcius temps \n",
    "t_u = torch.tensor(t_u) #unknown units \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759221c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37383205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume linear model \n",
    "# t_c = w * t_u + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5199dc71",
   "metadata": {},
   "source": [
    "- **Loss Function** is a function that **computes a single numerical** value that the learning process will attempt to minimize \n",
    "- we need to make sure the loss function make the loss positive both when the predicted value is greater than & when it is less than the true value. \n",
    "    - |predicted - true_value|\n",
    "    - (predicted - true_value)^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e06f278d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(t_u, w, b):\n",
    "    \"\"\"\n",
    "    Returns a tensor of predicted values using the inputs in a linear model. \n",
    "    t_u: Dependent variable. Input Tensor. Data we will use to predict the output t_c \n",
    "    w: Weight parameter. PyTorch scalar (zero-dimensional tensor)\n",
    "    b: Bias parameter (Offset parameter). The bias is what the output would be if all the inputs were zero. \n",
    "    \"\"\"\n",
    "\n",
    "    return w * t_u + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57f24b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(t_p, t_c): \n",
    "    \"\"\"\n",
    "    Mean square loss function. \n",
    "    t_p: predicted value\n",
    "    t_c: actual value\n",
    "    \"\"\"\n",
    "    squared_diffs = (t_p - t_c)**2\n",
    "    return squared_diffs.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e818fe9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([35.7000, 55.9000, 58.2000, 81.9000, 56.3000, 48.9000, 33.9000, 21.8000,\n",
       "        48.4000, 60.4000, 68.4000])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.ones(())\n",
    "b = torch.zeros(())\n",
    "t_p = model(t_u, w, b) \n",
    "t_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "841ecb70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1763.8848)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_fn(t_p, t_c)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428024d6",
   "metadata": {},
   "source": [
    "**Gradient Descent:** Compute the rate of change of the loss with respect to each parameter, and modify each parameter in the direction of decreasing loss. We can estimate the rate of change by adding a small number to `w` and `b` and see how much the loss changes in that neighborhood. \n",
    "\n",
    "**Gradient:** A vector of derivatives created by taking the individual derivatives of the loss with respect to each parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad36497a",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0.1 \n",
    "loss_rate_of_change_w = (loss_fn(model(t_u, w + delta, b), t_c ) - loss_fn(model(t_u, w-delta, b), t_c)) / (2.0 * delta) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2ab93f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_rate_of_change_b = (loss_fn(model(t_u, w, b+delta), t_c) - loss_fn(model(t_u, w, b-delta), t_c)) / (2.0 * delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "962491ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "w = w-learning_rate*loss_rate_of_change_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a24569e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dloss_fn(t_p, t_c): \n",
    "    dsq_diffs = 2*(t_p-t_c)/t_p.size(0) \n",
    "    return dsq_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c1e398a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dmodel_dw(t_u, w, b): \n",
    "    return t_u "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef77344c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dmodel_db(t_u, w, b):\n",
    "    return 1.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74d9f01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_fn(t_u, t_c, t_p, w, b):\n",
    "    dloss_dtp = dloss_fn(t_p, t_c) \n",
    "    dloss_dw = dloss_dtp * dmodel_dw(t_u, w, b)\n",
    "    dloss_db = dloss_dtp * dmodel_db(t_u, w, b) \n",
    "    return torch.stack([dloss_dw.sum(), dloss_db.sum()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a828393b",
   "metadata": {},
   "source": [
    "**Epoch:** A training iteration during which we update the parameters for all of our training samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42b1ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(\n",
    "        n_epochs: int, \n",
    "        learning_rate: float, \n",
    "        params: tuple[torch.tensor, torch.tensor], \n",
    "        t_u: torch.tensor, \n",
    "        t_c: torch.tensor \n",
    "        ):\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1): \n",
    "        w, b = params\n",
    "        t_un = t_u*.1\n",
    "\n",
    "        # forward pass \n",
    "        pred = model(t_un, w, b) \n",
    "        loss = loss_fn(pred, t_c)\n",
    "\n",
    "        # backward pass \n",
    "        grad = grad_fn(t_un, t_c, pred, w, b)\n",
    "\n",
    "        params = params - learning_rate * grad \n",
    "\n",
    "        print(f'Epoch {epoch}, Loss {float(loss)}, \\n\\t Gradients (w,b): {grad}')\n",
    "    return params \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a287b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 1763.884765625, \n",
      "\t Gradients (w,b): tensor([4517.2964,   82.6000])\n",
      "Epoch 2, Loss 323.09051513671875, \n",
      "\t Gradients (w,b): tensor([1859.5493,   35.7843])\n",
      "Epoch 3, Loss 78.92963409423828, \n",
      "\t Gradients (w,b): tensor([765.4666,  16.5122])\n",
      "Epoch 4, Loss 37.5528450012207, \n",
      "\t Gradients (w,b): tensor([315.0790,   8.5787])\n",
      "Epoch 5, Loss 30.540283203125, \n",
      "\t Gradients (w,b): tensor([129.6733,   5.3127])\n",
      "Epoch 6, Loss 29.351154327392578, \n",
      "\t Gradients (w,b): tensor([53.3495,  3.9682])\n",
      "Epoch 7, Loss 29.148883819580078, \n",
      "\t Gradients (w,b): tensor([21.9304,  3.4148])\n",
      "Epoch 8, Loss 29.113847732543945, \n",
      "\t Gradients (w,b): tensor([8.9964, 3.1869])\n",
      "Epoch 9, Loss 29.107145309448242, \n",
      "\t Gradients (w,b): tensor([3.6721, 3.0930])\n",
      "Epoch 10, Loss 29.105247497558594, \n",
      "\t Gradients (w,b): tensor([1.4803, 3.0544])\n",
      "Epoch 11, Loss 29.104167938232422, \n",
      "\t Gradients (w,b): tensor([0.5781, 3.0384])\n",
      "Epoch 12, Loss 29.103221893310547, \n",
      "\t Gradients (w,b): tensor([0.2066, 3.0318])\n",
      "Epoch 13, Loss 29.102294921875, \n",
      "\t Gradients (w,b): tensor([0.0537, 3.0291])\n",
      "Epoch 14, Loss 29.10137939453125, \n",
      "\t Gradients (w,b): tensor([-0.0093,  3.0279])\n",
      "Epoch 15, Loss 29.100465774536133, \n",
      "\t Gradients (w,b): tensor([-0.0353,  3.0274])\n",
      "Epoch 16, Loss 29.09954833984375, \n",
      "\t Gradients (w,b): tensor([-0.0459,  3.0272])\n",
      "Epoch 17, Loss 29.098630905151367, \n",
      "\t Gradients (w,b): tensor([-0.0502,  3.0270])\n",
      "Epoch 18, Loss 29.09771728515625, \n",
      "\t Gradients (w,b): tensor([-0.0520,  3.0270])\n",
      "Epoch 19, Loss 29.0967960357666, \n",
      "\t Gradients (w,b): tensor([-0.0528,  3.0269])\n",
      "Epoch 20, Loss 29.09588050842285, \n",
      "\t Gradients (w,b): tensor([-0.0531,  3.0268])\n",
      "Epoch 21, Loss 29.094959259033203, \n",
      "\t Gradients (w,b): tensor([-0.0533,  3.0268])\n",
      "Epoch 22, Loss 29.09404945373535, \n",
      "\t Gradients (w,b): tensor([-0.0533,  3.0267])\n",
      "Epoch 23, Loss 29.0931339263916, \n",
      "\t Gradients (w,b): tensor([-0.0533,  3.0267])\n",
      "Epoch 24, Loss 29.09221649169922, \n",
      "\t Gradients (w,b): tensor([-0.0533,  3.0266])\n",
      "Epoch 25, Loss 29.09130096435547, \n",
      "\t Gradients (w,b): tensor([-0.0533,  3.0266])\n",
      "Epoch 26, Loss 29.09038543701172, \n",
      "\t Gradients (w,b): tensor([-0.0533,  3.0265])\n",
      "Epoch 27, Loss 29.08946418762207, \n",
      "\t Gradients (w,b): tensor([-0.0533,  3.0265])\n",
      "Epoch 28, Loss 29.088550567626953, \n",
      "\t Gradients (w,b): tensor([-0.0532,  3.0264])\n",
      "Epoch 29, Loss 29.087635040283203, \n",
      "\t Gradients (w,b): tensor([-0.0533,  3.0264])\n",
      "Epoch 30, Loss 29.086713790893555, \n",
      "\t Gradients (w,b): tensor([-0.0533,  3.0263])\n",
      "Epoch 31, Loss 29.085803985595703, \n",
      "\t Gradients (w,b): tensor([-0.0532,  3.0262])\n",
      "Epoch 32, Loss 29.084888458251953, \n",
      "\t Gradients (w,b): tensor([-0.0533,  3.0262])\n",
      "Epoch 33, Loss 29.083967208862305, \n",
      "\t Gradients (w,b): tensor([-0.0533,  3.0261])\n",
      "Epoch 34, Loss 29.083057403564453, \n",
      "\t Gradients (w,b): tensor([-0.0533,  3.0261])\n",
      "Epoch 35, Loss 29.082141876220703, \n",
      "\t Gradients (w,b): tensor([-0.0532,  3.0260])\n",
      "Epoch 36, Loss 29.081220626831055, \n",
      "\t Gradients (w,b): tensor([-0.0533,  3.0260])\n",
      "Epoch 37, Loss 29.08030891418457, \n",
      "\t Gradients (w,b): tensor([-0.0533,  3.0259])\n",
      "Epoch 38, Loss 29.079389572143555, \n",
      "\t Gradients (w,b): tensor([-0.0532,  3.0259])\n",
      "Epoch 39, Loss 29.078474044799805, \n",
      "\t Gradients (w,b): tensor([-0.0533,  3.0258])\n",
      "Epoch 40, Loss 29.07756233215332, \n",
      "\t Gradients (w,b): tensor([-0.0533,  3.0258])\n",
      "Epoch 41, Loss 29.076648712158203, \n",
      "\t Gradients (w,b): tensor([-0.0533,  3.0257])\n",
      "Epoch 42, Loss 29.07573127746582, \n",
      "\t Gradients (w,b): tensor([-0.0532,  3.0257])\n",
      "Epoch 43, Loss 29.074811935424805, \n",
      "\t Gradients (w,b): tensor([-0.0533,  3.0256])\n",
      "Epoch 44, Loss 29.073894500732422, \n",
      "\t Gradients (w,b): tensor([-0.0533,  3.0256])\n",
      "Epoch 45, Loss 29.072980880737305, \n",
      "\t Gradients (w,b): tensor([-0.0533,  3.0255])\n",
      "Epoch 46, Loss 29.07206916809082, \n",
      "\t Gradients (w,b): tensor([-0.0533,  3.0254])\n",
      "Epoch 47, Loss 29.071147918701172, \n",
      "\t Gradients (w,b): tensor([-0.0533,  3.0254])\n",
      "Epoch 48, Loss 29.070234298706055, \n",
      "\t Gradients (w,b): tensor([-0.0533,  3.0253])\n",
      "Epoch 49, Loss 29.06932258605957, \n",
      "\t Gradients (w,b): tensor([-0.0533,  3.0253])\n",
      "Epoch 50, Loss 29.068401336669922, \n",
      "\t Gradients (w,b): tensor([-0.0532,  3.0252])\n",
      "Epoch 51, Loss 29.067485809326172, \n",
      "\t Gradients (w,b): tensor([-0.0533,  3.0252])\n",
      "Epoch 52, Loss 29.066566467285156, \n",
      "\t Gradients (w,b): tensor([-0.0533,  3.0251])\n",
      "Epoch 53, Loss 29.065656661987305, \n",
      "\t Gradients (w,b): tensor([-0.0533,  3.0251])\n",
      "Epoch 54, Loss 29.064741134643555, \n",
      "\t Gradients (w,b): tensor([-0.0533,  3.0250])\n",
      "Epoch 55, Loss 29.063825607299805, \n",
      "\t Gradients (w,b): tensor([-0.0532,  3.0250])\n",
      "Epoch 56, Loss 29.062910079956055, \n",
      "\t Gradients (w,b): tensor([-0.0533,  3.0249])\n",
      "Epoch 57, Loss 29.061994552612305, \n",
      "\t Gradients (w,b): tensor([-0.0532,  3.0249])\n",
      "Epoch 58, Loss 29.061079025268555, \n",
      "\t Gradients (w,b): tensor([-0.0533,  3.0248])\n",
      "Epoch 59, Loss 29.060169219970703, \n",
      "\t Gradients (w,b): tensor([-0.0533,  3.0248])\n",
      "Epoch 60, Loss 29.059247970581055, \n",
      "\t Gradients (w,b): tensor([-0.0533,  3.0247])\n",
      "Epoch 61, Loss 29.05833625793457, \n",
      "\t Gradients (w,b): tensor([-0.0533,  3.0247])\n",
      "Epoch 62, Loss 29.057415008544922, \n",
      "\t Gradients (w,b): tensor([-0.0534,  3.0246])\n",
      "Epoch 63, Loss 29.056507110595703, \n",
      "\t Gradients (w,b): tensor([-0.0533,  3.0245])\n",
      "Epoch 64, Loss 29.055585861206055, \n",
      "\t Gradients (w,b): tensor([-0.0532,  3.0245])\n",
      "Epoch 65, Loss 29.05467414855957, \n",
      "\t Gradients (w,b): tensor([-0.0533,  3.0244])\n",
      "Epoch 66, Loss 29.053760528564453, \n",
      "\t Gradients (w,b): tensor([-0.0533,  3.0244])\n",
      "Epoch 67, Loss 29.05284309387207, \n",
      "\t Gradients (w,b): tensor([-0.0533,  3.0243])\n",
      "Epoch 68, Loss 29.051929473876953, \n",
      "\t Gradients (w,b): tensor([-0.0532,  3.0243])\n",
      "Epoch 69, Loss 29.05101203918457, \n",
      "\t Gradients (w,b): tensor([-0.0533,  3.0242])\n",
      "Epoch 70, Loss 29.050098419189453, \n",
      "\t Gradients (w,b): tensor([-0.0532,  3.0242])\n",
      "Epoch 71, Loss 29.049182891845703, \n",
      "\t Gradients (w,b): tensor([-0.0533,  3.0241])\n",
      "Epoch 72, Loss 29.04827308654785, \n",
      "\t Gradients (w,b): tensor([-0.0533,  3.0241])\n",
      "Epoch 73, Loss 29.04734992980957, \n",
      "\t Gradients (w,b): tensor([-0.0532,  3.0240])\n",
      "Epoch 74, Loss 29.04644203186035, \n",
      "\t Gradients (w,b): tensor([-0.0533,  3.0240])\n",
      "Epoch 75, Loss 29.045530319213867, \n",
      "\t Gradients (w,b): tensor([-0.0532,  3.0239])\n",
      "Epoch 76, Loss 29.04461097717285, \n",
      "\t Gradients (w,b): tensor([-0.0533,  3.0239])\n",
      "Epoch 77, Loss 29.043699264526367, \n",
      "\t Gradients (w,b): tensor([-0.0533,  3.0238])\n",
      "Epoch 78, Loss 29.042783737182617, \n",
      "\t Gradients (w,b): tensor([-0.0533,  3.0238])\n",
      "Epoch 79, Loss 29.0418701171875, \n",
      "\t Gradients (w,b): tensor([-0.0533,  3.0237])\n",
      "Epoch 80, Loss 29.04095458984375, \n",
      "\t Gradients (w,b): tensor([-0.0532,  3.0236])\n",
      "Epoch 81, Loss 29.0400390625, \n",
      "\t Gradients (w,b): tensor([-0.0534,  3.0236])\n",
      "Epoch 82, Loss 29.039121627807617, \n",
      "\t Gradients (w,b): tensor([-0.0533,  3.0235])\n",
      "Epoch 83, Loss 29.038209915161133, \n",
      "\t Gradients (w,b): tensor([-0.0532,  3.0235])\n",
      "Epoch 84, Loss 29.037294387817383, \n",
      "\t Gradients (w,b): tensor([-0.0533,  3.0234])\n",
      "Epoch 85, Loss 29.036378860473633, \n",
      "\t Gradients (w,b): tensor([-0.0533,  3.0234])\n",
      "Epoch 86, Loss 29.035463333129883, \n",
      "\t Gradients (w,b): tensor([-0.0532,  3.0233])\n",
      "Epoch 87, Loss 29.03455352783203, \n",
      "\t Gradients (w,b): tensor([-0.0533,  3.0233])\n",
      "Epoch 88, Loss 29.03363609313965, \n",
      "\t Gradients (w,b): tensor([-0.0532,  3.0232])\n",
      "Epoch 89, Loss 29.03272247314453, \n",
      "\t Gradients (w,b): tensor([-0.0533,  3.0232])\n",
      "Epoch 90, Loss 29.031810760498047, \n",
      "\t Gradients (w,b): tensor([-0.0533,  3.0231])\n",
      "Epoch 91, Loss 29.030895233154297, \n",
      "\t Gradients (w,b): tensor([-0.0532,  3.0231])\n",
      "Epoch 92, Loss 29.02997589111328, \n",
      "\t Gradients (w,b): tensor([-0.0532,  3.0230])\n",
      "Epoch 93, Loss 29.02906608581543, \n",
      "\t Gradients (w,b): tensor([-0.0533,  3.0230])\n",
      "Epoch 94, Loss 29.02815055847168, \n",
      "\t Gradients (w,b): tensor([-0.0532,  3.0229])\n",
      "Epoch 95, Loss 29.02723503112793, \n",
      "\t Gradients (w,b): tensor([-0.0533,  3.0229])\n",
      "Epoch 96, Loss 29.026323318481445, \n",
      "\t Gradients (w,b): tensor([-0.0533,  3.0228])\n",
      "Epoch 97, Loss 29.025409698486328, \n",
      "\t Gradients (w,b): tensor([-0.0532,  3.0227])\n",
      "Epoch 98, Loss 29.024492263793945, \n",
      "\t Gradients (w,b): tensor([-0.0532,  3.0227])\n",
      "Epoch 99, Loss 29.023582458496094, \n",
      "\t Gradients (w,b): tensor([-0.0533,  3.0226])\n",
      "Epoch 100, Loss 29.022666931152344, \n",
      "\t Gradients (w,b): tensor([-0.0532,  3.0226])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2327, -0.0438])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop(\n",
    "    n_epochs=100, \n",
    "    learning_rate=1e-4,\n",
    "    params = torch.tensor([1.0, 0.0]),\n",
    "    t_u = t_u,\n",
    "    t_c = t_c\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6ffc65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CMI-BFRB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
