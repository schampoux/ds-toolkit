{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644162e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms \n",
    "data_path = './datasets/'\n",
    "cifar10 = datasets.CIFAR10(root = data_path, download=True, train=True)\n",
    "cifar10_val = datasets.CIFAR10(root = data_path, download=True, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73173900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the method resolution order of our dataset instance. \n",
    "# Notice that the dataset is returned as a subclass of torch.utils.data.dataset.Dataset base class. \n",
    "type(cifar10).__mro__, cifar10.__repr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a308f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot one of the images \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "img, label = cifar10[99]\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9cceca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = cifar10.classes\n",
    "class_names[label]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4109f8f",
   "metadata": {},
   "source": [
    "### Transform image data on instantiation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17bb8df",
   "metadata": {},
   "source": [
    "Now let's use the `transforms` module from torchvision to convert these PIL images to PyTorch tensors. \n",
    "\n",
    "- This module defines a set of composable function-like objects that can be passed as an argument to a torchvision dataset upon instantiation, and they perform transformations on the data after it is loaded but before it is returned by `__getitem__`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe9a6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms \n",
    "dir(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381bd71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_tensor = transforms.ToTensor()\n",
    "img_t = to_tensor(img)\n",
    "img_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db2ccdc",
   "metadata": {},
   "source": [
    "The image has been turned into a 3x32x32 tensor and therefore a 3-channel (RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfe34ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the dataset with the transform \n",
    "tensor_cifar10 = datasets.CIFAR10(root=data_path, train=True, download=False, transform = transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06c9c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t, _ = tensor_cifar10[99]\n",
    "type(img_t), type(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec535552",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t.dtype, img_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ad45b3",
   "metadata": {},
   "source": [
    "The `.ToTensor()` transform turns the data into a 32-bit floating-point per channel, scaling the values down to the range `0.0` to `1.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f907d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t.min(), img_t.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06998586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting img_t: a tensor of image data \n",
    "plt.imshow(img_t.permute(1,2,0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62e7631",
   "metadata": {},
   "source": [
    "Transforms can be chained using `transforms.Compose`. They can handle normalization and data augmentation transparently. \n",
    "- Each channel having zero mean and unitary standard deviation. \n",
    "- It is good practice to normalize the dataset so that each channel has zero mean and unitary standard deviation. \n",
    "    By choosing activation functions that are linear around `0` plus or minus `1`(or`2`), keeping the data in the same range means it's more likely that neurons have nonzero gradients and hence will learn sooner. \n",
    "- Also, Normalizing each channel so that it has the same distribution will ensure that channel information can be mixed and update through gradient descent using the same learning rate. \n",
    "- `transforms.Normalize` takes mean and stdev as arguments. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0deda0b7",
   "metadata": {},
   "source": [
    "### Normalizing our tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a84d921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using transforms.Normalize, we can compute the mean value and the standard deviation of each channel across the dataset \n",
    "# then apply the following transform: v_n[c] = (v[c] - mean[c]) / stdev[c]\n",
    "import torch \n",
    "\n",
    "imgs = torch.stack([img_t for img_t, _ in tensor_cifar10], dim=3)\n",
    "imgs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59df187c",
   "metadata": {},
   "source": [
    "`3` channels (RGB), with height `32`, width`32`, and `50000` of these (images). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c57645",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs.view(3,-1).mean(dim=1), imgs.view(3,-1).std(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b07b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms.Normalize(mean = (0.4914, 0.4822, 0.4465), std=(0.2470, 0.2435, 0.2616))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a90be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_cifar10 = datasets.CIFAR10(\n",
    "    root = data_path, \n",
    "    download=False, \n",
    "    train=True, \n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = (0.4914, 0.4822, 0.4465), std=(0.2470, 0.2435, 0.2616)),\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9c1ff4",
   "metadata": {},
   "source": [
    "Plotting the normalized image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37faff71",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t, label = transformed_cifar10[99]\n",
    "plt.imshow(img_t.permute(1,2,0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b94b9e",
   "metadata": {},
   "source": [
    "### Building the Dataset, testing the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1d3a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane', 'bird']\n",
    "label_map = {0:0, 2:1}\n",
    "cifar2 = [(img, label_map[label]) for img, label in cifar10 if label in [0,2]]\n",
    "cifar2_val = [(img, label_map[label]) for img, label in cifar10_val if label in [0,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6af6da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "n_out = 2\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(3072, 512),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(512, n_out),\n",
    "    nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4584861",
   "metadata": {},
   "source": [
    "Understanding that the output is categorical (airplane, bird), therefore we should use one-hot-encoding representation. In the ideal case, the network would output `torch.tensor([1.0, 0.0])` for an airplane, `torch.tensor([0.0, 1.0])` for a bird, but since our classifier will not be perfect, we can expect the network to output something in between. **We can interpret our output as probabilities i.e. the first entry is the probability of 'airplane', and the second is the probability of 'bird'.** \n",
    "- Each element of the output must be in the `[0.0, 1.0]` range. \n",
    "- The elements of the output must add up to 1.0. \n",
    "\n",
    "In other words, a probability of an outcome cannot be less than 0 or greater than 1, and we are certain that one of the two outcomes will occur. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83de428",
   "metadata": {},
   "source": [
    "**Softmax** is a function that takes a vector of values and produces another vector of the same dimension, where the values satisfy the constraints listed above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1feec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum()\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "softmax(x), softmax(x).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bf0422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "x = torch.tensor([[1.0,2.0,3.0],[1.0,2.0,3.0]])\n",
    "# in this example, each row is a different input vector\n",
    "# Apply softmax ALONG the columns. i.e. ALONG dimension 1. \n",
    "softmax = nn.Softmax(dim=1)\n",
    "softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5855ecf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, _ = cifar2[0]\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9663aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t_t = to_tensor(img)\n",
    "plt.imshow(img_t_t.permute(1,2,0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773a66b5",
   "metadata": {},
   "source": [
    "Note, our model expects 3072 features (3x32x32) in the input. Also, `nn` works with data organized into batches. So after we obtain a 1d tensor of length 3072, unsqueeze it to produce a batch dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fece69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model \n",
    "img_batch = img_t_t.view(-1).unsqueeze(0)\n",
    "img_batch.size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322c5fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(img_batch)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1b0d5a",
   "metadata": {},
   "source": [
    "The output is meaningless, because the weights and biases for our linear layers have not been trained. Their elements are initialized randomly by PyTorch between -1.0 and 1.0. \n",
    "\n",
    "Furthermore, the model is not aware of which output probability is which. The loss function associates meaning with these two numbers after backpropogation, and since the loss function was not run, there is no meaning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d728b301",
   "metadata": {},
   "source": [
    "### Loss Function for Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9476d2",
   "metadata": {},
   "source": [
    "After training, we will be able to get the label as an index by computing the argmax of the output probabilities. That is, the index at which we get the maximum probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9877747e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(out, dim=1)\n",
    "# index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70785e2c",
   "metadata": {},
   "source": [
    "We will be using the Negative Log Likelihood loss function, conveniently provided in the `nn` module. \n",
    "\n",
    "- We want to maximize the probability associated with the correct class (likelihood). Note that we are interested in ensuring that the correct classes probility is higher than the others (Winning the softmax ranking). We are **not** interested in driving this probability to 1 (like MSE). \n",
    "\n",
    "- In the following example, we have two predictions from our model, in which the correct classification of the input is associated with the second index of the output. \n",
    "\n",
    "    - If we have [0.40, 0.60] we want to maximize the likelihood of the model parameters\n",
    "    - If we have [0.60, 0.40] we want to maximize that likelihood of the model parameters\n",
    "\n",
    "    - In the first example, the likelihood (probability associated with the correct class) is larger than the likelihood of the first index, therefore, we want a low loss (penalty) for this correct classification. \n",
    "    - In the second example, the likelihood (probabilty associated with the correct class) of the second index is lower than the likelihood of the first index. Therefore, we want high loss to correct this misclassification. \n",
    "\n",
    "- From a loss perspective, we want to minimize the negative log-likelihood. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e7f22a",
   "metadata": {},
   "source": [
    "\n",
    "- Our input to the loss function needs to be a tensor of log probabilities. \n",
    "    - Therefore we should use `nn.LogSoftmax`\n",
    "        - Softmax providing us with the probabilities, \n",
    "        - Log providing us with a numerically stable Logarithm of these probabilities. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef7791e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's reqrite the model \n",
    "import torch.nn as nn \n",
    "model = nn.Sequential(\n",
    "    nn.Linear(3072, 512),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(512, 2),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")\n",
    "loss = nn.NLLLoss()\n",
    "img, label = cifar2[0]\n",
    "img = to_tensor(img)\n",
    "out = model(img.view(-1).unsqueeze(0))\n",
    "loss(out, torch.tensor([label]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92db870a",
   "metadata": {},
   "source": [
    "**Note**, we are introducing randomness in our gradient descent by estimating the gradient on a few samples at a time. We are working on small batches of shuffled data. It turns out that following gradients estimated over minibatches, which are poorer approximations of gradients estimated across the whole dataset, helps convergence and prevents the optimization process from getting stuck in local minima it encounters along the way. \n",
    "\n",
    "- Since gradients estimated over minibatches are poorer approximations of gradients estimated across the whole dataset, we want to use a reasonably small learning rate. \n",
    "- Shuffling the data at each epoch is an attempt to help ensure that the sequence of gradients (from the minibatch) is representative of the whole dataset. \n",
    "- Minibatches are typically a constant size that we need to set prior to training, just like the learning rate. (hyperparameters)\n",
    "- Below we choose minibatches of size 1. We can use the `Dataloader` module from `torch.utils.data.DataLoader` to help with shuffling and organizing the data into minibatches. \n",
    "- `DataLoader` provides with a range of different sampling strategies. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28174345",
   "metadata": {},
   "source": [
    "### `DataLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb760212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "584518b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './datasets/'\n",
    "cifar10 = datasets.CIFAR10(root = data_path, \n",
    "                           train=True, \n",
    "                           download=False, \n",
    "                           transform = transforms.Compose(\n",
    "                                    [transforms.ToTensor(), \n",
    "                                    transforms.Normalize(mean = (0.4914, 0.4822, 0.4465), std=(0.2470, 0.2435, 0.2616))]\n",
    "                            ))\n",
    "cifar10_val = datasets.CIFAR10(root = data_path, \n",
    "                               train=False, \n",
    "                               download=False, \n",
    "                                transform = transforms.Compose(\n",
    "                                    [transforms.ToTensor(), \n",
    "                                    transforms.Normalize(mean = (0.4914, 0.4822, 0.4465), std=(0.2470, 0.2435, 0.2616))]\n",
    "                            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68f027a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane', 'bird']\n",
    "label_map = {0:0, 2:1}\n",
    "\n",
    "# to_tensor = transforms.ToTensor()\n",
    "# cifar2 = [(to_tensor(img), label_map[label]) for img, label in cifar10 if label in [0,2]]\n",
    "# cifar2_val = [(to_tensor(img), label_map[label]) for img, label in cifar10_val if label in [0,2]]\n",
    "\n",
    "cifar2 = [(img, label_map[label]) for img, label in cifar10 if label in [0,2]]\n",
    "cifar2_val = [(img, label_map[label]) for img, label in cifar10_val if label in [0,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b74c4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c710227b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d11b5af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3072, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, label = cifar2[0]\n",
    "img.view(-1).unsqueeze(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a037f43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note, dataloader can be iterated over \n",
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True, pin_memory=True)\n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4e1037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition \n",
    "model = nn.Sequential(\n",
    "    nn.Linear(3072, 1024),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(1024, 512),    \n",
    "    nn.Tanh(),\n",
    "    nn.Linear(512, 128),    \n",
    "    nn.Tanh(),\n",
    "    nn.Linear(128, 2),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")\n",
    "\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.NLLLoss()\n",
    "n_epochs = 100 \n",
    "\n",
    "# Training Loop \n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    for imgs, labels in train_loader:\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = model(imgs.view(batch_size, -1))\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    print(f\"Epoch: {epoch}\", f\"Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1589809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate Prediction accuracy using validation set \n",
    "correct_predictions=0\n",
    "total=0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for img, label in val_loader:\n",
    "        input = img.view(64, -1)\n",
    "        if input.size() != torch.Size([64, 3072]):\n",
    "            continue\n",
    "        output = model(input)\n",
    "        max_likelihood, predicted_label = torch.max(output, dim=1) \n",
    "\n",
    "        total += label.shape[0]\n",
    "        correct_predictions += int((predicted_label == label).sum())\n",
    "\n",
    "    print((correct_predictions/total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf339163",
   "metadata": {},
   "source": [
    "We achieve roughly 80% prediction accuracy with The following architecture. \n",
    "\n",
    "- linear \n",
    "- activation \n",
    "- linear \n",
    "\n",
    "Next we attempt to taper the number of features more gently toward the output, in the hope that the intermediate layers will do a better job of squeezing information in increasingly shorter intermediate output. Here is the next architecture, which gave us 83% prediction accuracy:\n",
    "\n",
    "- linear 3072 x 1024\n",
    "- activation \n",
    "- linear 1024 x 512\n",
    "- activation\n",
    "- linear 512 x 128\n",
    "- activation \n",
    "- linear 128 x 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e78035",
   "metadata": {},
   "source": [
    "Since `nn.LogSoftmax()` into `nn.NLLLoss()` is the same as using `nn.CrossEntropyLoss()`, let's chage the model to use that. The only difference now is that `nn.CrossEntropyLoss` takes scores/logits, as opposed to computing log probabilities using `nn.LogSoftmax()`. In other words, ou rmodel will not ouput (log) probabilities, so if we want probabilities, we will have to pass the output through `nn.Softmax()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72007ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 0.4158\n",
      "Epoch: 1 Loss: 0.5512\n",
      "Epoch: 2 Loss: 0.4369\n",
      "Epoch: 3 Loss: 0.2352\n",
      "Epoch: 4 Loss: 0.4977\n",
      "Epoch: 5 Loss: 0.3340\n",
      "Epoch: 6 Loss: 0.6549\n",
      "Epoch: 7 Loss: 0.3093\n",
      "Epoch: 8 Loss: 0.3780\n",
      "Epoch: 9 Loss: 0.3420\n",
      "Epoch: 10 Loss: 0.4732\n",
      "Epoch: 11 Loss: 0.3006\n",
      "Epoch: 12 Loss: 0.5349\n",
      "Epoch: 13 Loss: 0.7789\n",
      "Epoch: 14 Loss: 0.5304\n",
      "Epoch: 15 Loss: 0.1487\n",
      "Epoch: 16 Loss: 0.4387\n",
      "Epoch: 17 Loss: 0.3058\n",
      "Epoch: 18 Loss: 0.2961\n",
      "Epoch: 19 Loss: 0.4051\n",
      "Epoch: 20 Loss: 0.3337\n",
      "Epoch: 21 Loss: 0.2484\n",
      "Epoch: 22 Loss: 0.3913\n",
      "Epoch: 23 Loss: 0.3757\n",
      "Epoch: 24 Loss: 0.2041\n",
      "Epoch: 25 Loss: 0.0762\n",
      "Epoch: 26 Loss: 0.3186\n",
      "Epoch: 27 Loss: 0.1389\n",
      "Epoch: 28 Loss: 0.7340\n",
      "Epoch: 29 Loss: 0.2120\n",
      "Epoch: 30 Loss: 0.8572\n",
      "Epoch: 31 Loss: 0.1143\n",
      "Epoch: 32 Loss: 0.2143\n",
      "Epoch: 33 Loss: 0.0343\n",
      "Epoch: 34 Loss: 0.1618\n",
      "Epoch: 35 Loss: 0.3736\n",
      "Epoch: 36 Loss: 0.1013\n",
      "Epoch: 37 Loss: 0.0572\n",
      "Epoch: 38 Loss: 0.0504\n",
      "Epoch: 39 Loss: 0.0530\n",
      "Epoch: 40 Loss: 0.0580\n",
      "Epoch: 41 Loss: 0.7399\n",
      "Epoch: 42 Loss: 0.0250\n",
      "Epoch: 43 Loss: 0.2159\n",
      "Epoch: 44 Loss: 0.0528\n",
      "Epoch: 45 Loss: 0.1482\n",
      "Epoch: 46 Loss: 0.0256\n",
      "Epoch: 47 Loss: 0.0136\n",
      "Epoch: 48 Loss: 0.0100\n",
      "Epoch: 49 Loss: 1.2750\n",
      "Epoch: 50 Loss: 0.3010\n",
      "Epoch: 51 Loss: 0.0146\n",
      "Epoch: 52 Loss: 0.0430\n",
      "Epoch: 53 Loss: 0.0182\n",
      "Epoch: 54 Loss: 0.0274\n",
      "Epoch: 55 Loss: 0.0030\n",
      "Epoch: 56 Loss: 0.0694\n",
      "Epoch: 57 Loss: 0.1964\n",
      "Epoch: 58 Loss: 0.0053\n",
      "Epoch: 59 Loss: 0.0202\n",
      "Epoch: 60 Loss: 0.0327\n",
      "Epoch: 61 Loss: 0.0074\n",
      "Epoch: 62 Loss: 0.0250\n",
      "Epoch: 63 Loss: 0.0011\n",
      "Epoch: 64 Loss: 0.0197\n",
      "Epoch: 65 Loss: 0.0017\n",
      "Epoch: 66 Loss: 0.0023\n",
      "Epoch: 67 Loss: 0.0078\n",
      "Epoch: 68 Loss: 0.0133\n",
      "Epoch: 69 Loss: 0.0080\n",
      "Epoch: 70 Loss: 0.0021\n",
      "Epoch: 71 Loss: 0.0015\n",
      "Epoch: 72 Loss: 0.0024\n",
      "Epoch: 73 Loss: 0.0062\n",
      "Epoch: 74 Loss: 0.0016\n",
      "Epoch: 75 Loss: 0.0173\n",
      "Epoch: 76 Loss: 0.0034\n",
      "Epoch: 77 Loss: 0.0026\n",
      "Epoch: 78 Loss: 0.0083\n",
      "Epoch: 79 Loss: 0.0030\n",
      "Epoch: 80 Loss: 0.0053\n",
      "Epoch: 81 Loss: 0.0014\n",
      "Epoch: 82 Loss: 0.0008\n",
      "Epoch: 83 Loss: 0.0012\n",
      "Epoch: 84 Loss: 0.0838\n",
      "Epoch: 85 Loss: 0.0021\n",
      "Epoch: 86 Loss: 0.0019\n",
      "Epoch: 87 Loss: 0.0021\n",
      "Epoch: 88 Loss: 0.0022\n",
      "Epoch: 89 Loss: 0.0035\n",
      "Epoch: 90 Loss: 0.0017\n",
      "Epoch: 91 Loss: 0.0023\n",
      "Epoch: 92 Loss: 0.0013\n",
      "Epoch: 93 Loss: 0.0289\n",
      "Epoch: 94 Loss: 0.0010\n",
      "Epoch: 95 Loss: 0.0018\n",
      "Epoch: 96 Loss: 0.0022\n",
      "Epoch: 97 Loss: 0.0024\n",
      "Epoch: 98 Loss: 0.0010\n",
      "Epoch: 99 Loss: 0.0014\n"
     ]
    }
   ],
   "source": [
    "# Model Definition \n",
    "model = nn.Sequential(\n",
    "    nn.Linear(3072, 1024),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(1024, 512),    \n",
    "    nn.Tanh(),\n",
    "    nn.Linear(512, 128),    \n",
    "    nn.Tanh(),\n",
    "    nn.Linear(128, 2),\n",
    ").to(device)\n",
    "\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "n_epochs = 100 \n",
    "\n",
    "# Training Loop \n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    for imgs, labels in train_loader:\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = model(imgs.view(batch_size, -1))\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch: {epoch}\", f\"Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c7cc0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8024193548387096\n"
     ]
    }
   ],
   "source": [
    "# calculate Prediction accuracy using validation set \n",
    "correct_predictions=0\n",
    "total=0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for img, label in val_loader:\n",
    "\n",
    "        # move validation tensors to GPU \n",
    "        img = img.to(device, non_blocking=True)\n",
    "        label = label.to(device, non_blocking=True)\n",
    "\n",
    "        input = img.view(64, -1)\n",
    "        if input.size() != torch.Size([64, 3072]):\n",
    "            continue\n",
    "        \n",
    "        output = model(input)\n",
    "        max_likelihood, predicted_label = torch.max(output, dim=1) \n",
    "\n",
    "        total += label.shape[0]\n",
    "        correct_predictions += int((predicted_label == label).sum())\n",
    "\n",
    "    print((correct_predictions/total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022bdc48",
   "metadata": {},
   "source": [
    "Note, our loss during training is almost zero, but our validation set prediction accuracy is only 80%. What is happening here? We are overfitting on the training data, the model is to complex. The model is 'memorizing' the training data rather than learning it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8183e203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3737474, [3145728, 1024, 524288, 512, 65536, 128, 256, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numel_list = [p.numel() for p in model.parameters()if p.requires_grad==True]\n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a115db5",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eebe80",
   "metadata": {},
   "source": [
    "We have a model, a dataset, and a training loop, and our model learns. However, due to a mismatch between our problem and our netqoek structure, we end up overfitting our training data, rather than learning the generalized features of what we want the model to detect. \n",
    "\n",
    "We have created a model that allowsfor relating every pixel to every other pixelin the image, regardless of their spatial arrangement. We have a reasonable assumption that pixels that are closer together are in theory alot more related though. This means we are training a classifier that is not translation-invariant, so we're forced to use a lot of capacity for learning translated replicas if we want to hope to do well on the validation set. The solution to our current set of problems is to change our model to use convolutional layers.\n",
    "\n",
    "We have been treating 2D images as 1D data. \n",
    "\n",
    "**Translation invariance** means a models prediction does not change much when the input is shifted in space. Since our model is not translation invariant, it will not be able to confirm that an airplane in the bottom right of an image is the same as an airplane in the top left of an image. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4b1f48",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CMI-BFRB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
